<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Massively Scalable Inverse Reinforcement Learning in Google Maps</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Massively Scalable Inverse Reinforcement Learning in Google Maps" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="/" />
<meta property="og:url" content="/" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Massively Scalable Inverse Reinforcement Learning in Google Maps" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","headline":"Massively Scalable Inverse Reinforcement Learning in Google Maps","url":"/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" /></head>
<body>

    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <header class="post-header"><h1 class="page-heading">Massively Scalable Inverse Reinforcement Learning in Google Maps</h1><p class="post-meta">
                <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                  <span class="p-author h-card" itemprop="name">Matt Barnes<sup>1</sup></span></span>, 
                <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                  <span class="p-author h-card" itemprop="name">Matthew Abueg<sup>1</sup></span></span>, 
                <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                  <span class="p-author h-card" itemprop="name">Oliver F. Lange<sup>2</sup></span></span>, 
                <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                  <span class="p-author h-card" itemprop="name">Matt Deeds<sup>2</sup></span></span>, 
                <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                  <span class="p-author h-card" itemprop="name">Jason Trader<sup>2</sup></span></span>, 
                <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                  <span class="p-author h-card" itemprop="name">Denali Molitor<sup>1</sup></span></span>, 
                <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                  <span class="p-author h-card" itemprop="name">Markus Wulfmeier<sup>3</sup></span></span>, 
                <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                  <span class="p-author h-card" itemprop="name">Shawn O'Banion<sup>1</sup></span></span><br>
                <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                  <span class="p-author h-card" itemprop="name"><sup>1</sup>Google Research</span></span>, 
                <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                  <span class="p-author h-card" itemprop="name"><sup>2</sup>Google Maps</span></span>, 
                <span itemprop="author" itemscope itemtype="http://schema.org/Person">
                  <span class="p-author h-card" itemprop="name"><sup>3</sup>Google DeepMind</span></span></p>
                <form action="https://arxiv.org/pdf/2305.11290.pdf">
                  <button style="button">
                    PDF
                  </button>
                </form>
              </p>
        </header>

        <p><img src="/assets/worldwide-lift.png" alt="Image name" />
<em>Google Maps route accuracy improvements in several world regions, when using our inverse reinforcement learning policy RHIP.</em></p>

<p><br /></p>
<h2 id="abstract">Abstract</h2>
<p>Optimizing for humansâ€™ latent preferences is a grand challenge in route recommendation, where globally-scalable solutions remain an open problem. Although past work created increasingly general solutions for the application of inverse reinforcement learning (IRL), these have not been successfully scaled to world-sized MDPs, large datasets, and highly parameterized models; respectively hundreds of millions of states, trajectories, and parameters. In this work, we surpass previous limitations through a series of advancements focused on graph compression, parallelization, and problem initialization based on dominant eigenvectors. We introduce Receding Horizon Inverse Planning (RHIP), which generalizes existing work and enables control of key performance trade-offs via its planning horizon. Our policy achieves a 16-24% improvement in global route quality, and, to our knowledge, represents the largest instance of IRL in a real-world setting to date. Our results show critical benefits to more sustainable modes of transportation (e.g. two-wheelers), where factors beyond journey time (e.g. route safety) play a substantial role. We conclude with ablations of key components, negative results on state-of-the-art eigenvalue solvers, and identify future opportunities to improve scalability via IRL-specific batching strategies.</p>

<h2 id="bibtex">BibTeX</h2>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{Barnes2023,
  title = {Massively Scalable Inverse Reinforcement Learning in Google Maps},
  author = {Barnes, Matt and Abueg, Matthew and Lange, Oliver F and Deeds, Matt and Trader, Jason and Molitor, Denali and Wulfmeier, Markus and O'Banion, Shawn},
  journal = {arXiv preprint},
  year = {2023}
}
</code></pre></div></div>

      </div>
    </main>

  </body>

</html>
